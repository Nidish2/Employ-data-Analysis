{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "MODEL_SELECTION_OUTPUT_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/model_selection'\n",
    "\n",
    "# File paths for datasets\n",
    "TRAIN_FEATURES_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_features.csv'\n",
    "TEST_FEATURES_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_features.csv'\n",
    "TRAIN_TARGET_ATTRITION_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_target_attrition.csv'\n",
    "TEST_TARGET_ATTRITION_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_target_attrition.csv'\n",
    "TRAIN_TARGET_STATUS_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_target_status.csv'\n",
    "TEST_TARGET_STATUS_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_target_status.csv'\n",
    "\n",
    "# Pickle files for preprocessing\n",
    "SCALER_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/models/scaler.pkl'\n",
    "LABEL_ENCODER_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/models/label_encoder.pkl'\n",
    "\n",
    "# Load scaler and label encoder\n",
    "scaler = joblib.load(SCALER_PATH)\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (39722, 42), X_test shape: (9931, 42)\n",
      "y_train_attrition shape: (39722,), y_test_attrition shape: (9931,)\n",
      "y_train_status shape: (39722,), y_test_status shape: (9931,)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "X_train = pd.read_csv(TRAIN_FEATURES_PATH)\n",
    "X_test = pd.read_csv(TEST_FEATURES_PATH)\n",
    "y_train_attrition = pd.read_csv(TRAIN_TARGET_ATTRITION_PATH).squeeze()  # Squeeze to 1D array\n",
    "y_test_attrition = pd.read_csv(TEST_TARGET_ATTRITION_PATH).squeeze()\n",
    "y_train_status = pd.read_csv(TRAIN_TARGET_STATUS_PATH).squeeze()\n",
    "y_test_status = pd.read_csv(TEST_TARGET_STATUS_PATH).squeeze()\n",
    "\n",
    "# Confirm data shapes\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train_attrition shape: {y_train_attrition.shape}, y_test_attrition shape: {y_test_attrition.shape}\")\n",
    "print(f\"y_train_status shape: {y_train_status.shape}, y_test_status shape: {y_test_status.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'XGBoost': GradientBoostingClassifier(random_state=42),  # XGBoost equivalent for scikit-learn\n",
    "    'LightGBM': GradientBoostingClassifier(random_state=42)  # LightGBM equivalent for scikit-learn\n",
    "}\n",
    "\n",
    "# Parameters for hyperparameter tuning\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, X_train, y_train, X_test, y_test, param_grid=None):\n",
    "    \"\"\"Train, tune, and evaluate the model.\"\"\"\n",
    "    # Hyperparameter tuning\n",
    "    if param_grid:\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Best score for {model_name}: {grid_search.best_score_}\")\n",
    "    else:\n",
    "        best_model = model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1], multi_class='ovr') if hasattr(best_model, \"predict_proba\") else \"N/A\"\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Save results\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'ROC AUC Score': roc_auc,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Classification Report': classification_rep\n",
    "    }\n",
    "\n",
    "    # Save confusion matrix as image\n",
    "    os.makedirs(MODEL_SELECTION_OUTPUT_PATH, exist_ok=True)\n",
    "    conf_matrix_df = pd.DataFrame(conf_matrix, index=np.unique(y_test), columns=np.unique(y_test))\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_df, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(MODEL_SELECTION_OUTPUT_PATH + f'/{model_name}_confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features after scaling: Index(['EmployeeID', 'age', 'length_of_service', 'department_name',\n",
      "       'job_title', 'store_name', 'gender_full', 'STATUS_YEAR',\n",
      "       'GENERAL APPEARANCE', 'MANNER OF SPEAKING', 'PHYSICAL CONDITION',\n",
      "       'MENTAL ALERTNESS', 'SELF-CONFIDENCE', 'ABILITY TO PRESENT IDEAS',\n",
      "       'COMMUNICATION SKILLS', 'Student Performance Rating', 'salary',\n",
      "       'performance_score', 'manager_rating', 'self_rating',\n",
      "       'work_life_balance_score', 'overtime_hours', 'working_hours',\n",
      "       'employee_satisfaction_score', 'salary_hike_percent',\n",
      "       'post_promotion_performance', 'peer_feedback_score', 'absenteeism_rate',\n",
      "       'tenure_bucket', 'months_since_last_promotion', 'turnover_risk_index',\n",
      "       'engagement_index', 'absenteeism_category', 'performance_improvement'],\n",
      "      dtype='object')\n",
      "Categorical features after encoding: Index(['orighiredate_key', 'terminationdate_key', 'city_name',\n",
      "       'termreason_desc', 'termtype_desc', 'BUSINESS_UNIT', 'CLASS',\n",
      "       'last_promotion_date'],\n",
      "      dtype='object')\n",
      "Evaluating models for ATTRITION...\n",
      "Best parameters for RandomForest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score for RandomForest: 0.7980967718016043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for DecisionTree: {'max_depth': 10, 'min_samples_split': 5}\n",
      "Best score for DecisionTree: 0.788882831171251\n",
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best score for XGBoost: 0.7980967718016043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best score for LightGBM: 0.7980967718016043\n",
      "Evaluating models for STATUS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best score for RandomForest: 0.7275817126430008\n",
      "Best parameters for DecisionTree: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Best score for DecisionTree: 0.7175367883554928\n",
      "Best parameters for XGBoost: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best score for XGBoost: 0.7275817126430008\n",
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best score for LightGBM: 0.7275817126430008\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Paths to the data files\n",
    "TRAIN_FEATURES_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_features.csv'\n",
    "TEST_FEATURES_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_features.csv'\n",
    "TRAIN_TARGET_ATTRITION_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_target_attrition.csv'\n",
    "TEST_TARGET_ATTRITION_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_target_attrition.csv'\n",
    "TRAIN_TARGET_STATUS_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/train_target_status.csv'\n",
    "TEST_TARGET_STATUS_PATH = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/data/test_target_status.csv'\n",
    "\n",
    "# Load datasets\n",
    "X_train = pd.read_csv(TRAIN_FEATURES_PATH)\n",
    "X_test = pd.read_csv(TEST_FEATURES_PATH)\n",
    "y_train_attrition = pd.read_csv(TRAIN_TARGET_ATTRITION_PATH).squeeze()  # Squeeze to 1D array\n",
    "y_test_attrition = pd.read_csv(TEST_TARGET_ATTRITION_PATH).squeeze()\n",
    "y_train_status = pd.read_csv(TRAIN_TARGET_STATUS_PATH).squeeze()\n",
    "y_test_status = pd.read_csv(TEST_TARGET_STATUS_PATH).squeeze()\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Instantiate the scaler and fit it on the train data for numerical columns\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[numerical_columns])\n",
    "\n",
    "print(f\"Numerical features after scaling: {numerical_columns}\")\n",
    "\n",
    "# Apply scaling to both train and test data for numerical columns\n",
    "X_train[numerical_columns] = scaler.transform(X_train[numerical_columns])\n",
    "X_test[numerical_columns] = scaler.transform(X_test[numerical_columns])\n",
    "\n",
    "\n",
    "# Initialize LabelEncoders for each categorical column\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_columns}\n",
    "\n",
    "# Fit encoders on training data and transform both train and test data\n",
    "for col in categorical_columns:\n",
    "    # Fit the encoder on training data\n",
    "    label_encoders[col].fit(X_train[col].astype(str))\n",
    "    \n",
    "    # Transform training data\n",
    "    X_train[col] = label_encoders[col].transform(X_train[col].astype(str))\n",
    "    \n",
    "    # Transform test data with a fallback for unseen labels\n",
    "    X_test[col] = X_test[col].apply(\n",
    "        lambda x: label_encoders[col].transform([x])[0] \n",
    "        if x in label_encoders[col].classes_ else -1\n",
    "    )\n",
    "\n",
    "print(f\"Categorical features after encoding: {categorical_columns}\")\n",
    "\n",
    "# Initialize results lists\n",
    "results_attrition = []\n",
    "results_status = []\n",
    "\n",
    "# Evaluate models for ATTRITION target\n",
    "print(\"Evaluating models for ATTRITION...\")\n",
    "for model_name, model in models.items():\n",
    "    results_attrition.append(evaluate_model(\n",
    "        model_name, model, X_train, y_train_attrition, X_test, y_test_attrition, param_grids.get(model_name)\n",
    "    ))\n",
    "\n",
    "# Evaluate models for STATUS target\n",
    "print(\"Evaluating models for STATUS...\")\n",
    "for model_name, model in models.items():\n",
    "    results_status.append(evaluate_model(\n",
    "        model_name, model, X_train, y_train_status, X_test, y_test_status, param_grids.get(model_name)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_classification_report(output_path, report, model_name, target_variable):\n",
    "    \"\"\"Save classification report to a text file.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    report_file = os.path.join(output_path, f\"{model_name}_{target_variable}_classification_report.txt\")\n",
    "    with open(report_file, \"w\") as file:\n",
    "        file.write(report)\n",
    "    print(f\"Classification report saved for {model_name} ({target_variable}).\")\n",
    "\n",
    "def save_confusion_matrix(matrix, model_name, target_variable, output_path):\n",
    "    \"\"\"Save confusion matrix as CSV.\"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    conf_matrix_file = os.path.join(output_path, f\"{model_name}_{target_variable}_confusion_matrix.csv\")\n",
    "    matrix.to_csv(conf_matrix_file, index=True)\n",
    "    print(f\"Confusion matrix saved for {model_name} ({target_variable}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results for ATTRITION...\n",
      "Classification report saved for RandomForest (Attrition).\n",
      "Confusion matrix saved for RandomForest (Attrition).\n",
      "Classification report saved for DecisionTree (Attrition).\n",
      "Confusion matrix saved for DecisionTree (Attrition).\n",
      "Classification report saved for XGBoost (Attrition).\n",
      "Confusion matrix saved for XGBoost (Attrition).\n",
      "Classification report saved for LightGBM (Attrition).\n",
      "Confusion matrix saved for LightGBM (Attrition).\n",
      "Saving results for STATUS...\n",
      "Classification report saved for RandomForest (Status).\n",
      "Confusion matrix saved for RandomForest (Status).\n",
      "Classification report saved for DecisionTree (Status).\n",
      "Confusion matrix saved for DecisionTree (Status).\n",
      "Classification report saved for XGBoost (Status).\n",
      "Confusion matrix saved for XGBoost (Status).\n",
      "Classification report saved for LightGBM (Status).\n",
      "Confusion matrix saved for LightGBM (Status).\n"
     ]
    }
   ],
   "source": [
    "# Save results for ATTRITION\n",
    "print(\"Saving results for ATTRITION...\")\n",
    "for result in results_attrition:\n",
    "    model_name = result['Model']\n",
    "    # Save classification report\n",
    "    save_classification_report(MODEL_SELECTION_OUTPUT_PATH, result['Classification Report'], model_name, \"Attrition\")\n",
    "    \n",
    "    # Save confusion matrix as CSV\n",
    "    conf_matrix_df = pd.DataFrame(\n",
    "        result['Confusion Matrix'], \n",
    "        index=label_encoder.classes_, \n",
    "        columns=label_encoder.classes_\n",
    "    )\n",
    "    save_confusion_matrix(conf_matrix_df, model_name, \"Attrition\", MODEL_SELECTION_OUTPUT_PATH)\n",
    "\n",
    "# Save results for STATUS\n",
    "print(\"Saving results for STATUS...\")\n",
    "for result in results_status:\n",
    "    model_name = result['Model']\n",
    "    # Save classification report\n",
    "    save_classification_report(MODEL_SELECTION_OUTPUT_PATH, result['Classification Report'], model_name, \"Status\")\n",
    "    \n",
    "    # Save confusion matrix as CSV\n",
    "    conf_matrix_df = pd.DataFrame(\n",
    "        result['Confusion Matrix'], \n",
    "        index=label_encoder.classes_, \n",
    "        columns=label_encoder.classes_\n",
    "    )\n",
    "    save_confusion_matrix(conf_matrix_df, model_name, \"Status\", MODEL_SELECTION_OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance comparison saved for both targets.\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrames for export\n",
    "attrition_performance_df = pd.DataFrame(results_attrition).drop(columns=['Confusion Matrix', 'Classification Report'])\n",
    "status_performance_df = pd.DataFrame(results_status).drop(columns=['Confusion Matrix', 'Classification Report'])\n",
    "\n",
    "# Save model performances to CSV\n",
    "attrition_performance_file = os.path.join(MODEL_SELECTION_OUTPUT_PATH, \"attrition_model_performance_comparison.csv\")\n",
    "status_performance_file = os.path.join(MODEL_SELECTION_OUTPUT_PATH, \"status_model_performance_comparison.csv\")\n",
    "\n",
    "attrition_performance_df.to_csv(attrition_performance_file, index=False)\n",
    "status_performance_df.to_csv(status_performance_file, index=False)\n",
    "\n",
    "print(\"Model performance comparison saved for both targets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_roc_auc_plot(model, X_test, y_test, model_name, target_variable, output_path):\n",
    "    \"\"\"Save ROC AUC curve as an image.\"\"\"\n",
    "    if not hasattr(model, \"predict_proba\"):\n",
    "        print(f\"Model {model_name} does not support predict_proba. Skipping ROC AUC plot.\")\n",
    "        return\n",
    "    \n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    # Compute ROC curve and AUC\n",
    "    probas = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC AUC - {model_name} ({target_variable})')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Save plot\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    plot_file = os.path.join(output_path, f\"{model_name}_{target_variable}_roc_auc.png\")\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    print(f\"ROC AUC plot saved for {model_name} ({target_variable}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ROC AUC plots for ATTRITION...\n",
      "Skipping model due to missing information in results_attrition: {'Model': 'RandomForest', 'Accuracy': 0.798106937871312, 'Precision': np.float64(0.6369746842783223), 'ROC AUC Score': np.float64(0.48909661878611566), 'Confusion Matrix': array([[7926,    0],\n",
      "       [2005,    0]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n         0.0       0.80      1.00      0.89      7926\\n         1.0       0.00      0.00      0.00      2005\\n\\n    accuracy                           0.80      9931\\n   macro avg       0.40      0.50      0.44      9931\\nweighted avg       0.64      0.80      0.71      9931\\n'}\n",
      "Skipping model due to missing information in results_attrition: {'Model': 'DecisionTree', 'Accuracy': 0.7869298157285268, 'Precision': np.float64(0.6720246009661082), 'ROC AUC Score': np.float64(0.501584670672549), 'Confusion Matrix': array([[7785,  141],\n",
      "       [1975,   30]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n         0.0       0.80      0.98      0.88      7926\\n         1.0       0.18      0.01      0.03      2005\\n\\n    accuracy                           0.79      9931\\n   macro avg       0.49      0.50      0.45      9931\\nweighted avg       0.67      0.79      0.71      9931\\n'}\n",
      "Skipping model due to missing information in results_attrition: {'Model': 'XGBoost', 'Accuracy': 0.798106937871312, 'Precision': np.float64(0.6369746842783223), 'ROC AUC Score': np.float64(0.488854667519946), 'Confusion Matrix': array([[7926,    0],\n",
      "       [2005,    0]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n         0.0       0.80      1.00      0.89      7926\\n         1.0       0.00      0.00      0.00      2005\\n\\n    accuracy                           0.80      9931\\n   macro avg       0.40      0.50      0.44      9931\\nweighted avg       0.64      0.80      0.71      9931\\n'}\n",
      "Skipping model due to missing information in results_attrition: {'Model': 'LightGBM', 'Accuracy': 0.798106937871312, 'Precision': np.float64(0.6369746842783223), 'ROC AUC Score': np.float64(0.488854667519946), 'Confusion Matrix': array([[7926,    0],\n",
      "       [2005,    0]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n         0.0       0.80      1.00      0.89      7926\\n         1.0       0.00      0.00      0.00      2005\\n\\n    accuracy                           0.80      9931\\n   macro avg       0.40      0.50      0.44      9931\\nweighted avg       0.64      0.80      0.71      9931\\n'}\n",
      "Generating ROC AUC plots for STATUS...\n",
      "Skipping model due to missing information in results_status: {'Model': 'RandomForest', 'Accuracy': 0.7286275299567012, 'Precision': np.float64(0.8036518519492115), 'ROC AUC Score': np.float64(0.6420476602702834), 'Confusion Matrix': array([[6930,    1],\n",
      "       [2694,  306]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.72      1.00      0.84      6931\\n           1       1.00      0.10      0.19      3000\\n\\n    accuracy                           0.73      9931\\n   macro avg       0.86      0.55      0.51      9931\\nweighted avg       0.80      0.73      0.64      9931\\n'}\n",
      "Skipping model due to missing information in results_status: {'Model': 'DecisionTree', 'Accuracy': 0.7221830631356358, 'Precision': np.float64(0.7243468749471966), 'ROC AUC Score': np.float64(0.6251799403645457), 'Confusion Matrix': array([[6790,  141],\n",
      "       [2618,  382]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.72      0.98      0.83      6931\\n           1       0.73      0.13      0.22      3000\\n\\n    accuracy                           0.72      9931\\n   macro avg       0.73      0.55      0.52      9931\\nweighted avg       0.72      0.72      0.65      9931\\n'}\n",
      "Skipping model due to missing information in results_status: {'Model': 'XGBoost', 'Accuracy': 0.7286275299567012, 'Precision': np.float64(0.8036518519492115), 'ROC AUC Score': np.float64(0.6346511325926995), 'Confusion Matrix': array([[6930,    1],\n",
      "       [2694,  306]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.72      1.00      0.84      6931\\n           1       1.00      0.10      0.19      3000\\n\\n    accuracy                           0.73      9931\\n   macro avg       0.86      0.55      0.51      9931\\nweighted avg       0.80      0.73      0.64      9931\\n'}\n",
      "Skipping model due to missing information in results_status: {'Model': 'LightGBM', 'Accuracy': 0.7286275299567012, 'Precision': np.float64(0.8036518519492115), 'ROC AUC Score': np.float64(0.6346511325926995), 'Confusion Matrix': array([[6930,    1],\n",
      "       [2694,  306]]), 'Classification Report': '              precision    recall  f1-score   support\\n\\n           0       0.72      1.00      0.84      6931\\n           1       1.00      0.10      0.19      3000\\n\\n    accuracy                           0.73      9931\\n   macro avg       0.86      0.55      0.51      9931\\nweighted avg       0.80      0.73      0.64      9931\\n'}\n"
     ]
    }
   ],
   "source": [
    "def save_roc_auc_plot(model, X_test, y_test, model_name, target_variable, output_path):\n",
    "    \"\"\"\n",
    "    Save ROC AUC curve as an image.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Trained model.\n",
    "        X_test: Test feature data.\n",
    "        y_test: True labels for test data.\n",
    "        model_name: Name of the model.\n",
    "        target_variable: Target variable being evaluated (e.g., \"Attrition\" or \"Status\").\n",
    "        output_path: Directory where the plot will be saved.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "\n",
    "    if not hasattr(model, \"predict_proba\"):\n",
    "        print(f\"Model {model_name} does not support predict_proba. Skipping ROC AUC plot.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Compute probabilities and ROC curve\n",
    "        probas = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, probas)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC AUC - {model_name} ({target_variable})')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # Save plot\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        plot_file = os.path.join(output_path, f\"{model_name}_{target_variable}_roc_auc.png\")\n",
    "        plt.savefig(plot_file)\n",
    "        plt.close()\n",
    "        print(f\"ROC AUC plot saved for {model_name} ({target_variable}) at {plot_file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating ROC AUC plot for {model_name} ({target_variable}): {e}\")\n",
    "\n",
    "\n",
    "# Save ROC AUC plots for ATTRITION\n",
    "print(\"Generating ROC AUC plots for ATTRITION...\")\n",
    "for result in results_attrition:\n",
    "    model_name = result.get('Model')\n",
    "    model = result.get('Trained Model')  # Retrieve the trained model safely\n",
    "    if model and model_name:\n",
    "        save_roc_auc_plot(model, X_test, y_test_attrition, model_name, \"Attrition\", MODEL_SELECTION_OUTPUT_PATH)\n",
    "    else:\n",
    "        print(f\"Skipping model due to missing information in results_attrition: {result}\")\n",
    "\n",
    "# Save ROC AUC plots for STATUS\n",
    "print(\"Generating ROC AUC plots for STATUS...\")\n",
    "for result in results_status:\n",
    "    model_name = result.get('Model')\n",
    "    model = result.get('Trained Model')  # Retrieve the trained model safely\n",
    "    if model and model_name:\n",
    "        save_roc_auc_plot(model, X_test, y_test_status, model_name, \"Status\", MODEL_SELECTION_OUTPUT_PATH)\n",
    "    else:\n",
    "        print(f\"Skipping model due to missing information in results_status: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving y_true and y_pred for Attrition...\n",
      "Combined (y_true, y_pred) saved for RandomForest (Attrition).\n",
      "y_true saved for RandomForest (Attrition).\n",
      "y_pred saved for RandomForest (Attrition).\n",
      "Combined (y_true, y_pred) saved for DecisionTree (Attrition).\n",
      "y_true saved for DecisionTree (Attrition).\n",
      "y_pred saved for DecisionTree (Attrition).\n",
      "Combined (y_true, y_pred) saved for XGBoost (Attrition).\n",
      "y_true saved for XGBoost (Attrition).\n",
      "y_pred saved for XGBoost (Attrition).\n",
      "Combined (y_true, y_pred) saved for LightGBM (Attrition).\n",
      "y_true saved for LightGBM (Attrition).\n",
      "y_pred saved for LightGBM (Attrition).\n",
      "Saving y_true and y_pred for Status...\n",
      "Combined (y_true, y_pred) saved for RandomForest (Status).\n",
      "y_true saved for RandomForest (Status).\n",
      "y_pred saved for RandomForest (Status).\n",
      "Combined (y_true, y_pred) saved for DecisionTree (Status).\n",
      "y_true saved for DecisionTree (Status).\n",
      "y_pred saved for DecisionTree (Status).\n",
      "Combined (y_true, y_pred) saved for XGBoost (Status).\n",
      "y_true saved for XGBoost (Status).\n",
      "y_pred saved for XGBoost (Status).\n",
      "Combined (y_true, y_pred) saved for LightGBM (Status).\n",
      "y_true saved for LightGBM (Status).\n",
      "y_pred saved for LightGBM (Status).\n"
     ]
    }
   ],
   "source": [
    "# Fit and save y_true and y_pred for Attrition\n",
    "print(\"Saving y_true and y_pred for Attrition...\")\n",
    "for result in results_attrition:  # Use results from earlier evaluation\n",
    "    model_name = result['Model']\n",
    "    model = models[model_name]  # Retrieve the model instance\n",
    "    y_true = y_test_attrition  # Actual target values for Attrition\n",
    "    # Ensure the model is fitted by evaluating it on Attrition data\n",
    "    model.fit(X_train, y_train_attrition)\n",
    "    y_pred = model.predict(X_test)  # Predictions\n",
    "    save_y_true_y_pred_combined_and_individual(y_true, y_pred, model_name, \"Attrition\", MODEL_SELECTION_OUTPUT_PATH)\n",
    "\n",
    "# Fit and save y_true and y_pred for Status\n",
    "print(\"Saving y_true and y_pred for Status...\")\n",
    "for result in results_status:  # Use results from earlier evaluation\n",
    "    model_name = result['Model']\n",
    "    model = models[model_name]  # Retrieve the model instance\n",
    "    y_true = y_test_status  # Actual target values for Status\n",
    "    # Ensure the model is fitted by evaluating it on Status data\n",
    "    model.fit(X_train, y_train_status)\n",
    "    y_pred = model.predict(X_test)  # Predictions\n",
    "    save_y_true_y_pred_combined_and_individual(y_true, y_pred, model_name, \"Status\", MODEL_SELECTION_OUTPUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
