{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import dalex as dx\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Set output directory\n",
    "OUTPUT_DIR = r\"C:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\outputs\\predictive_analytics\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = r\"C:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\data\\transformed_data.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Define target columns\n",
    "promotion_target = \"promotion\"  # Create this target column if not present\n",
    "performance_target = \"post_promotion_performance\"\n",
    "\n",
    "# Ensure numeric encoding for categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Check for missing values\n",
    "df.fillna(df.median(), inplace=True)  # Replace missing values with median for simplicity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target for promotion prediction (example logic)\n",
    "df[promotion_target] = (df['months_since_last_promotion'] > 12).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target for promotion prediction\n",
    "X_promotion = df.drop(columns=[promotion_target, performance_target])\n",
    "y_promotion = df[promotion_target]\n",
    "\n",
    "# Train-Test split\n",
    "X_train_promotion, X_test_promotion, y_train_promotion, y_test_promotion = train_test_split(\n",
    "    X_promotion, y_promotion, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target for performance prediction\n",
    "X_performance = df.drop(columns=[promotion_target, performance_target])\n",
    "y_performance = df[performance_target]\n",
    "\n",
    "# Train-Test split\n",
    "X_train_performance, X_test_performance, y_train_performance, y_test_performance = train_test_split(\n",
    "    X_performance, y_performance, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": LGBMClassifier()\n",
    "}\n",
    "\n",
    "# Define pipeline\n",
    "def create_pipeline(model):\n",
    "    return Pipeline([\n",
    "        ('scaler', StandardScaler()), \n",
    "        ('classifier', model)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models for promotion prediction...\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6132\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n",
      "Model: Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6132\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6132\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:27:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6132\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 14913, number of negative: 24809\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2765\n",
      "[LightGBM] [Info] Number of data points in the train set: 39722, number of used features: 43\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.375434 -> initscore=-0.508973\n",
      "[LightGBM] [Info] Start training from score -0.508973\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Model: LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      6132\n",
      "           1       1.00      1.00      1.00      3799\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "promotion_results = []\n",
    "\n",
    "print(\"Evaluating models for promotion prediction...\")\n",
    "for model_name, model in models.items():\n",
    "    pipeline = create_pipeline(model)\n",
    "    pipeline.fit(X_train_promotion, y_train_promotion)\n",
    "    \n",
    "    # Predictions and evaluations\n",
    "    y_pred = pipeline.predict(X_test_promotion)\n",
    "    y_proba = pipeline.predict_proba(X_test_promotion)[:, 1] if hasattr(pipeline, \"predict_proba\") else None\n",
    "    roc_auc = roc_auc_score(y_test_promotion, y_proba) if y_proba is not None else None\n",
    "    report = classification_report(y_test_promotion, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test_promotion, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    promotion_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Classification Report\": report,\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"Model: {model_name}\\n{report}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Mapping: {np.int64(0): np.int64(0), np.int64(1): np.int64(1), np.int64(2): np.int64(2), np.int64(3): np.int64(3), np.int64(4): np.int64(4), np.int64(5): np.int64(5), np.int64(6): np.int64(6), np.int64(7): np.int64(7), np.int64(8): np.int64(8), np.int64(9): np.int64(9), np.int64(10): np.int64(10), np.int64(11): np.int64(11), np.int64(12): np.int64(12), np.int64(13): np.int64(13), np.int64(14): np.int64(14), np.int64(15): np.int64(15), np.int64(16): np.int64(16), np.int64(17): np.int64(17), np.int64(18): np.int64(18), np.int64(19): np.int64(19), np.int64(20): np.int64(20)}\n",
      "Evaluating models for performance prediction...\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.01      0.02       266\n",
      "           1       0.48      0.95      0.63       525\n",
      "           2       0.40      0.42      0.41       485\n",
      "           3       0.40      0.36      0.38       489\n",
      "           4       0.51      0.35      0.41       488\n",
      "           5       0.45      0.46      0.45       463\n",
      "           6       0.43      0.43      0.43       486\n",
      "           7       0.46      0.43      0.45       493\n",
      "           8       0.41      0.38      0.40       483\n",
      "           9       0.50      0.55      0.52       513\n",
      "          10       0.49      0.33      0.40       466\n",
      "          11       0.53      0.49      0.51       524\n",
      "          12       0.41      0.37      0.39       495\n",
      "          13       0.35      0.38      0.36       457\n",
      "          14       0.44      0.55      0.49       517\n",
      "          15       0.56      0.39      0.46       527\n",
      "          16       0.53      0.43      0.47       499\n",
      "          17       0.48      0.51      0.50       506\n",
      "          18       0.47      0.39      0.43       524\n",
      "          19       0.46      0.98      0.63       483\n",
      "          20       0.33      0.00      0.01       242\n",
      "\n",
      "    accuracy                           0.46      9931\n",
      "   macro avg       0.47      0.44      0.42      9931\n",
      "weighted avg       0.47      0.46      0.44      9931\n",
      "\n",
      "\n",
      "ROC AUC: 0.9618060909170306\n",
      "\n",
      "Model: Decision Tree\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       266\n",
      "           1       0.99      0.98      0.98       525\n",
      "           2       0.99      1.00      0.99       485\n",
      "           3       1.00      1.00      1.00       489\n",
      "           4       1.00      1.00      1.00       488\n",
      "           5       1.00      1.00      1.00       463\n",
      "           6       1.00      1.00      1.00       486\n",
      "           7       1.00      1.00      1.00       493\n",
      "           8       1.00      1.00      1.00       483\n",
      "           9       1.00      1.00      1.00       513\n",
      "          10       1.00      1.00      1.00       466\n",
      "          11       1.00      0.99      1.00       524\n",
      "          12       1.00      1.00      1.00       495\n",
      "          13       1.00      1.00      1.00       457\n",
      "          14       1.00      1.00      1.00       517\n",
      "          15       1.00      1.00      1.00       527\n",
      "          16       1.00      1.00      1.00       499\n",
      "          17       1.00      1.00      1.00       506\n",
      "          18       1.00      1.00      1.00       524\n",
      "          19       1.00      1.00      1.00       483\n",
      "          20       1.00      1.00      1.00       242\n",
      "\n",
      "    accuracy                           1.00      9931\n",
      "   macro avg       1.00      1.00      1.00      9931\n",
      "weighted avg       1.00      1.00      1.00      9931\n",
      "\n",
      "\n",
      "ROC AUC: 0.9987158680604774\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.08      0.13       266\n",
      "           1       0.25      0.33      0.28       525\n",
      "           2       0.18      0.27      0.22       485\n",
      "           3       0.13      0.16      0.14       489\n",
      "           4       0.14      0.15      0.15       488\n",
      "           5       0.17      0.20      0.18       463\n",
      "           6       0.14      0.16      0.15       486\n",
      "           7       0.14      0.16      0.15       493\n",
      "           8       0.14      0.12      0.13       483\n",
      "           9       0.16      0.16      0.16       513\n",
      "          10       0.19      0.18      0.18       466\n",
      "          11       0.16      0.13      0.14       524\n",
      "          12       0.17      0.17      0.17       495\n",
      "          13       0.15      0.17      0.16       457\n",
      "          14       0.16      0.16      0.16       517\n",
      "          15       0.18      0.14      0.16       527\n",
      "          16       0.21      0.20      0.20       499\n",
      "          17       0.19      0.18      0.19       506\n",
      "          18       0.24      0.20      0.22       524\n",
      "          19       0.29      0.27      0.28       483\n",
      "          20       0.42      0.04      0.08       242\n",
      "\n",
      "    accuracy                           0.18      9931\n",
      "   macro avg       0.20      0.17      0.17      9931\n",
      "weighted avg       0.19      0.18      0.18      9931\n",
      "\n",
      "\n",
      "ROC AUC: 0.7429534946202659\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:37:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       266\n",
      "           1       0.98      0.99      0.99       525\n",
      "           2       0.94      0.98      0.96       485\n",
      "           3       0.91      0.90      0.91       489\n",
      "           4       0.86      0.90      0.88       488\n",
      "           5       0.82      0.76      0.79       463\n",
      "           6       0.80      0.86      0.83       486\n",
      "           7       0.84      0.83      0.84       493\n",
      "           8       0.75      0.75      0.75       483\n",
      "           9       0.71      0.69      0.70       513\n",
      "          10       0.64      0.67      0.65       466\n",
      "          11       0.70      0.59      0.64       524\n",
      "          12       0.68      0.60      0.64       495\n",
      "          13       0.72      0.77      0.74       457\n",
      "          14       0.80      0.82      0.81       517\n",
      "          15       0.87      0.87      0.87       527\n",
      "          16       0.89      0.92      0.90       499\n",
      "          17       0.94      0.97      0.96       506\n",
      "          18       0.98      0.98      0.98       524\n",
      "          19       0.96      0.99      0.98       483\n",
      "          20       1.00      0.95      0.98       242\n",
      "\n",
      "    accuracy                           0.84      9931\n",
      "   macro avg       0.85      0.85      0.85      9931\n",
      "weighted avg       0.84      0.84      0.84      9931\n",
      "\n",
      "\n",
      "ROC AUC: 0.9926694925608961\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2765\n",
      "[LightGBM] [Info] Number of data points in the train set: 39722, number of used features: 43\n",
      "[LightGBM] [Info] Start training from score -3.680906\n",
      "[LightGBM] [Info] Start training from score -2.999314\n",
      "[LightGBM] [Info] Start training from score -2.972885\n",
      "[LightGBM] [Info] Start training from score -3.013051\n",
      "[LightGBM] [Info] Start training from score -3.023349\n",
      "[LightGBM] [Info] Start training from score -2.988258\n",
      "[LightGBM] [Info] Start training from score -2.981286\n",
      "[LightGBM] [Info] Start training from score -2.971409\n",
      "[LightGBM] [Info] Start training from score -3.011004\n",
      "[LightGBM] [Info] Start training from score -2.986760\n",
      "[LightGBM] [Info] Start training from score -3.025422\n",
      "[LightGBM] [Info] Start training from score -3.008961\n",
      "[LightGBM] [Info] Start training from score -2.990760\n",
      "[LightGBM] [Info] Start training from score -2.967975\n",
      "[LightGBM] [Info] Start training from score -2.950981\n",
      "[LightGBM] [Info] Start training from score -3.000831\n",
      "[LightGBM] [Info] Start training from score -3.001337\n",
      "[LightGBM] [Info] Start training from score -2.986261\n",
      "[LightGBM] [Info] Start training from score -3.023867\n",
      "[LightGBM] [Info] Start training from score -3.022315\n",
      "[LightGBM] [Info] Start training from score -3.690946\n",
      "Model: LightGBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       266\n",
      "           1       0.98      0.98      0.98       525\n",
      "           2       0.95      0.99      0.97       485\n",
      "           3       0.91      0.93      0.92       489\n",
      "           4       0.90      0.89      0.90       488\n",
      "           5       0.88      0.91      0.89       463\n",
      "           6       0.82      0.84      0.83       486\n",
      "           7       0.71      0.81      0.76       493\n",
      "           8       0.73      0.66      0.69       483\n",
      "           9       0.75      0.67      0.71       513\n",
      "          10       0.68      0.56      0.61       466\n",
      "          11       0.69      0.68      0.68       524\n",
      "          12       0.72      0.66      0.69       495\n",
      "          13       0.72      0.84      0.77       457\n",
      "          14       0.86      0.91      0.88       517\n",
      "          15       0.92      0.90      0.91       527\n",
      "          16       0.92      0.93      0.92       499\n",
      "          17       0.95      0.98      0.96       506\n",
      "          18       0.99      1.00      1.00       524\n",
      "          19       1.00      1.00      1.00       483\n",
      "          20       1.00      1.00      1.00       242\n",
      "\n",
      "    accuracy                           0.86      9931\n",
      "   macro avg       0.86      0.86      0.86      9931\n",
      "weighted avg       0.86      0.86      0.86      9931\n",
      "\n",
      "\n",
      "ROC AUC: 0.9931396790111657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert y_train_performance and y_test_performance to integer labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_performance = label_encoder.fit_transform(y_train_performance)\n",
    "y_test_performance = label_encoder.transform(y_test_performance)\n",
    "\n",
    "# Convert class names to strings for classification_report\n",
    "class_names = [str(class_) for class_ in label_encoder.classes_]\n",
    "\n",
    "# Print the mapping for verification\n",
    "class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Class Mapping:\", class_mapping)\n",
    "\n",
    "# Proceed with model training\n",
    "performance_results = []\n",
    "\n",
    "print(\"Evaluating models for performance prediction...\")\n",
    "for model_name, model in models.items():\n",
    "    pipeline = create_pipeline(model)\n",
    "    pipeline.fit(X_train_performance, y_train_performance)\n",
    "    \n",
    "    # Predictions and evaluations\n",
    "    y_pred = pipeline.predict(X_test_performance)\n",
    "    y_proba = pipeline.predict_proba(X_test_performance) if hasattr(pipeline, \"predict_proba\") else None\n",
    "    \n",
    "    # Handle ROC AUC for binary and multiclass cases\n",
    "    if y_proba is not None:\n",
    "        if len(set(y_test_performance)) > 2:  # Multiclass case\n",
    "            roc_auc = roc_auc_score(y_test_performance, y_proba, multi_class=\"ovr\")\n",
    "        else:  # Binary case\n",
    "            roc_auc = roc_auc_score(y_test_performance, y_proba[:, 1])\n",
    "    else:\n",
    "        roc_auc = None\n",
    "    \n",
    "    report = classification_report(y_test_performance, y_pred, target_names=class_names)\n",
    "    conf_matrix = confusion_matrix(y_test_performance, y_pred)\n",
    "    \n",
    "    # Save results\n",
    "    performance_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Classification Report\": report,\n",
    "        \"Confusion Matrix\": conf_matrix,\n",
    "        \"ROC AUC\": roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"Model: {model_name}\\n{report}\\n\")\n",
    "    if roc_auc is not None:\n",
    "        print(f\"ROC AUC: {roc_auc}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explaining model: Decision Tree\n",
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 39722 rows 43 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 39722 values\n",
      "  -> model_class       : sklearn.tree._classes.DecisionTreeClassifier (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_proba_default at 0x000001AEF30479C0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n",
      "  -> predicted values  : min = 0.0, mean = 0.0, max = 0.0\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = 0.0, mean = 0.375, max = 1.0\n",
      "  -> model_info        : package sklearn\n",
      "\n",
      "A new explainer has been created!\n",
      "Warning: Zero division error occurred for model Decision Tree when calculating performance metrics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation for Decision Tree saved at:\n",
      "- DALEX: None\n",
      "- LIME: C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/predictive_analytics/explanations\\Decision Tree_promotion_lime_explanation_7111.png\n",
      "Explaining model: Random Forest\n",
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 39722 rows 43 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 39722 values\n",
      "  -> model_class       : sklearn.ensemble._forest.RandomForestClassifier (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_proba_default at 0x000001AEF30479C0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> predicted values  : min = 0.05, mean = 0.118, max = 0.17\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHRI\\Documents\\DS\\DS_Projects\\Employment_Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> residuals         : min = -0.16, mean = 0.258, max = 0.94\n",
      "  -> model_info        : package sklearn\n",
      "\n",
      "A new explainer has been created!\n",
      "Warning: Zero division error occurred for model Random Forest when calculating performance metrics.\n",
      "Explanation for Random Forest saved at:\n",
      "- DALEX: None\n",
      "- LIME: C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/predictive_analytics/explanations\\Random Forest_promotion_lime_explanation_3711.png\n",
      "Explaining model: XGBoost\n",
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 39722 rows 43 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 39722 values\n",
      "  -> model_class       : xgboost.sklearn.XGBClassifier (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_proba_default at 0x000001AEF30479C0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n",
      "  -> predicted values  : min = 0.0914, mean = 0.124, max = 0.155\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = -0.155, mean = 0.251, max = 0.909\n",
      "  -> model_info        : package xgboost\n",
      "\n",
      "A new explainer has been created!\n",
      "Warning: Zero division error occurred for model XGBoost when calculating performance metrics.\n",
      "Explanation for XGBoost saved at:\n",
      "- DALEX: None\n",
      "- LIME: C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/predictive_analytics/explanations\\XGBoost_promotion_lime_explanation_6671.png\n",
      "Explaining model: LightGBM\n",
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 39722 rows 43 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.Series. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 39722 values\n",
      "  -> model_class       : lightgbm.sklearn.LGBMClassifier (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_proba_default at 0x000001AEF30479C0> will be used (default)\n",
      "  -> predict function  : Accepts pandas.DataFrame and numpy.ndarray.\n",
      "  -> predicted values  : min = 0.0564, mean = 0.0924, max = 0.102\n",
      "  -> model type        : classification will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         : min = -0.102, mean = 0.283, max = 0.944\n",
      "  -> model_info        : package lightgbm\n",
      "\n",
      "A new explainer has been created!\n",
      "Warning: Zero division error occurred for model LightGBM when calculating performance metrics.\n",
      "Explanation for LightGBM saved at:\n",
      "- DALEX: None\n",
      "- LIME: C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/predictive_analytics/explanations\\LightGBM_promotion_lime_explanation_9265.png\n"
     ]
    }
   ],
   "source": [
    "# Import DALEX and LIME libraries\n",
    "import dalex as dx\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define OUTPUT_DIR for saving explanation outputs\n",
    "OUTPUT_DIR = 'C:/Users/SHRI/Documents/DS/DS_Projects/Employment_Analysis/outputs/predictive_analytics/explanations'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Function to explain models using DALEX and LIME\n",
    "def explain_model(model, X_train, X_test, y_train, y_test, model_name, target=\"promotion\"):\n",
    "    # DALEX explainability\n",
    "    explainer = dx.Explainer(model, X_train, y_train)  # Correct DALEX instantiation\n",
    "    \n",
    "    # Try generating model explainability in DALEX, with error handling\n",
    "    try:\n",
    "        daex_model_path = os.path.join(OUTPUT_DIR, f\"{model_name}_{target}_dalex_explanation.html\")\n",
    "        \n",
    "        # Plot and save model performance, info, and profile (DALEX)\n",
    "        explainer.model_performance().plot()\n",
    "        explainer.model_info().plot()\n",
    "        explainer.model_profile().plot()\n",
    "\n",
    "        # Save DALEX explanation as an HTML file\n",
    "        explainer.model_performance().save(daex_model_path)\n",
    "    except ZeroDivisionError:\n",
    "        print(f\"Warning: Zero division error occurred for model {model_name} when calculating performance metrics.\")\n",
    "        daex_model_path = None  # Handle the error gracefully and return None\n",
    "\n",
    "    # LIME explainability\n",
    "    lime_explainer = LimeTabularExplainer(\n",
    "        X_train.values,  # The training data\n",
    "        mode='classification',\n",
    "        training_labels=y_train.values,\n",
    "        feature_names=X_train.columns,\n",
    "        class_names=[f'Not {target}', target],  # Customize class names\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "    \n",
    "    # Choose a random instance for explanation\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    instance = X_test.iloc[idx]\n",
    "    \n",
    "    # Explain the instance using LIME\n",
    "    lime_explanation = lime_explainer.explain_instance(\n",
    "        instance.values, model.predict_proba, num_features=10\n",
    "    )\n",
    "    \n",
    "    # Save the LIME explanation plot\n",
    "    lime_explanation_image_path = os.path.join(OUTPUT_DIR, f\"{model_name}_{target}_lime_explanation_{idx}.png\")\n",
    "    \n",
    "    # Generate and save the explanation figure using pyplot\n",
    "    lime_explanation.as_pyplot_figure()\n",
    "    plt.savefig(lime_explanation_image_path)\n",
    "    plt.close()  # Close the plot after saving\n",
    "    \n",
    "    return daex_model_path, lime_explanation_image_path\n",
    "\n",
    "# Assuming models are already defined and trained, and your data is prepared\n",
    "# Loop through all models and apply DALEX and LIME explainability\n",
    "explanation_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Skip Logistic Regression or any model with compatibility issues with DALEX\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        continue\n",
    "\n",
    "    print(f\"Explaining model: {model_name}\")\n",
    "    \n",
    "    # Explain the model using DALEX and LIME for the promotion prediction task\n",
    "    daex_path, lime_path = explain_model(\n",
    "        model, X_train_promotion, X_test_promotion, y_train_promotion, y_test_promotion, model_name, target=\"promotion\"\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    explanation_results[model_name] = {\n",
    "        \"DALEX\": daex_path,\n",
    "        \"LIME\": lime_path\n",
    "    }\n",
    "\n",
    "    print(f\"Explanation for {model_name} saved at:\\n- DALEX: {daex_path}\\n- LIME: {lime_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
